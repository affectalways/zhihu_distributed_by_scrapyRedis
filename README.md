# zhihu_distributed_by_scrapyRedis
利用scrapy_redis爬取知乎，分布式爬虫

利用技术：
    1.scrapy_redis
    2.mysql存储数据
    3.redis存储headers，cookies，zhihu:start_urls
    4.Spider是继承的RedisSpider
    



   


